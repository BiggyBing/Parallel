{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bingyangwen/anaconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Python optimisation variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import AdamOptimizer\n",
    "#tf.train.AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_shape(gradient):\n",
    "    '''\n",
    "    Record the parameters original shape\n",
    "    Input: the return value of optimizer.compute_gradients (many matrices)\n",
    "    Output: A list of shape\n",
    "    \n",
    "    '''\n",
    "    gradient = np.array(gradient)\n",
    "    #shape_list record parameter shapes of each layer\n",
    "    shape_list = []\n",
    "    for i in range(gradient.shape[0]):\n",
    "        grad_temp = gradient[i,0].flatten()\n",
    "        shape_list.append(['Layer_'+str(i+1), gradient[i,0].shape, grad_temp.shape[0]])\n",
    "        \n",
    "    return shape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_collector(gradient):\n",
    "    '''\n",
    "    Collect the gradient of each batch\n",
    "    Input: the return value of optimizer.compute_gradients (many matrices)\n",
    "    Output: the sum of gradients within one epoch as a vector\n",
    "    \n",
    "    '''\n",
    "    gradient = np.array(gradient)\n",
    "    #shape_list record parameter shapes of each layer\n",
    "    gradient_vector = []\n",
    "    for i in range(gradient.shape[0]):\n",
    "        grad_temp = gradient[i,0].flatten()\n",
    "        gradient_vector.append(grad_temp)\n",
    "        \n",
    "    return np.array(gradient_vector)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_parameter_collector(gradient):\n",
    "    '''\n",
    "    Collect the gradient of each batch\n",
    "    Input: the return value of optimizer.compute_gradients (many matrices)\n",
    "    Output: the sum of gradients within one epoch as a vector\n",
    "    \n",
    "    '''\n",
    "    gradient = np.array(gradient)\n",
    "    #shape_list record parameter shapes of each layer\n",
    "    parameter_vector = []\n",
    "    for i in range(gradient.shape[0]):\n",
    "        grad_temp = gradient[i,1].flatten()\n",
    "        parameter_vector.append(grad_temp)\n",
    "        \n",
    "    return np.array(parameter_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name):\n",
    "    # setup the filter input shape for tf.nn.conv_2d\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,\n",
    "                      num_filters]\n",
    "\n",
    "    # initialise weights and bias for the filter\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
    "                                      name=name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "\n",
    "    # setup the convolutional layer operation\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # add the bias\n",
    "    out_layer += bias\n",
    "\n",
    "    # apply a ReLU non-linear activation\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "\n",
    "    # now perform max pooling\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, \n",
    "                               padding='SAME')\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/bingyangwen/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/bingyangwen/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/bingyangwen/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/bingyangwen/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/bingyangwen/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer_Bing(AdamOptimizer):\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
    "               use_locking=False, name=\"Adam\"):\n",
    "\n",
    "        super(AdamOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        # Tensor versions of the constructor arguments, created in _prepare().\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = None\n",
    "        self._beta2_t = None\n",
    "        self._epsilon_t = None\n",
    "\n",
    "        # Created in SparseApply if needed.\n",
    "        self._updated_lr = None\n",
    "    \n",
    "    def minimize(self, loss, global_step=None, var_list=None,\n",
    "               gate_gradients=1, aggregation_method=None,\n",
    "               colocate_gradients_with_ops=False, name=None,\n",
    "               grad_loss=None):\n",
    "        \"\"\"\n",
    "        The same as function minimize, but return the result of compute_gradients\n",
    "        Created by: Big Bing in 7/28 \n",
    "        Purpose: To realize parallel computing(communicate gradient)\n",
    "        \"\"\"\n",
    "        grads_and_vars = self.compute_gradients(\n",
    "            loss, var_list=var_list, gate_gradients=gate_gradients,\n",
    "            aggregation_method=aggregation_method,\n",
    "            colocate_gradients_with_ops=colocate_gradients_with_ops,\n",
    "            grad_loss=grad_loss)[-8:]\n",
    "\n",
    "        vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n",
    "        if not vars_with_grad:\n",
    "          raise ValueError(\n",
    "              \"No gradients provided for any variable, check your graph for ops\"\n",
    "              \" that do not support gradients, between variables %s and loss %s.\" %\n",
    "              ([str(v) for _, v in grads_and_vars], loss))\n",
    "        #self.apply_gradients(grads_and_vars, global_step=global_step, name=name)\n",
    "\n",
    "        return self.apply_gradients(grads_and_vars, global_step=global_step, name=name),grads_and_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "epochs = 2\n",
    "batch_size = 50\n",
    "\n",
    "# declare the training data placeholders\n",
    "# input x - for 28 x 28 pixels = 784 - this is the flattened image data that is drawn from \n",
    "# mnist.train.nextbatch()\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# dynamically reshape the input\n",
    "x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "# now declare the output data placeholder - 10 digits\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    def __init__(self):\n",
    "        self.pred = 0\n",
    "        self.loss = 0\n",
    "        self.optimizer_gradient = 0\n",
    "        self.optimizer = 0\n",
    "        self.shape_list = 0\n",
    "        self.init_op = 0\n",
    "        \n",
    "        self.t = 0\n",
    "        self.t_0 = 0\n",
    "        self.grad = 0\n",
    "        self.learning_rate = 0.0001\n",
    "        self.torque = 0\n",
    "        self.belta = 0\n",
    "        self.resource = 0\n",
    "        self.grad_t0 = 0\n",
    "        self.history = []\n",
    "        self.w = 0\n",
    "        self.w_hat = 0\n",
    "        self.w_t0 = 0\n",
    "        \n",
    "    def Rec_from_Agg(self, w_global, torque_global):\n",
    "        self.w_t0 = w_global\n",
    "        self.w_hat = w_global\n",
    "        self.torque = torque_global\n",
    "    def Snd_to_Agg(self):\n",
    "        if self.t_0 > 0:\n",
    "            return w,self.resource, self.belta, self.grad_t0\n",
    "        else:\n",
    "            return w,self.resource\n",
    "    def aa(self):\n",
    "        self.w = self.w_hat\n",
    "    \n",
    "    def Est_Resource(self):\n",
    "        return self.resource\n",
    "\n",
    "    def get_coef(self):\n",
    "        return self.w\n",
    "    \n",
    "    def set_coef(self, w_global):\n",
    "        self.w = w_global\n",
    "        \n",
    "    def Est_Belta(self ,X, y):\n",
    "        grad_global_parameter = np.dot((np.dot(X ,self.w_hat)-y), X)# In time t, the gradient of local loss of global parameters\n",
    "        self.grad_t0 = grad_global_parameter\n",
    "        self.belta = np.linalg.norm(self.grad - grad_global_parameter)/np.linalg.norm(self.w - self.w_hat)\n",
    "        \n",
    "    def time_record(self):\n",
    "        self.t_0 = self.t\n",
    "        \n",
    "    def CNN_Layer(self):\n",
    "        x = tf.placeholder(tf.float32, [None, 784])\n",
    "        # dynamically reshape the input\n",
    "        x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        # now declare the output data placeholder - 10 digits\n",
    "        y = tf.placeholder(tf.float32, [None, 10])\n",
    "        # create some convolutional layers\n",
    "        layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1')\n",
    "        layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')\n",
    "\n",
    "        flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])\n",
    "        \n",
    "        # setup some weights and bias values for this layer, then activate with ReLU\n",
    "        wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1')\n",
    "        bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
    "        dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "        dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "        # another layer with softmax activations\n",
    "        wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
    "        bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
    "        dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "        \n",
    "        pred = tf.nn.softmax(dense_layer2)\n",
    "        #loss is cross_entropy loss\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2, labels=y))\n",
    "        \n",
    "        \n",
    "        return loss,pred\n",
    "    \n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, [None, 784])\n",
    "        # dynamically reshape the input\n",
    "        x_shaped = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        # now declare the output data placeholder - 10 digits\n",
    "        y = tf.placeholder(tf.float32, [None, 10])\n",
    "        # create some convolutional layers\n",
    "        layer1 = create_new_conv_layer(x_shaped, 1, 32, [5, 5], [2, 2], name='layer1')\n",
    "        layer2 = create_new_conv_layer(layer1, 32, 64, [5, 5], [2, 2], name='layer2')\n",
    "\n",
    "        flattened = tf.reshape(layer2, [-1, 7 * 7 * 64])\n",
    "        \n",
    "        # setup some weights and bias values for this layer, then activate with ReLU\n",
    "        wd1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1000], stddev=0.03), name='wd1')\n",
    "        bd1 = tf.Variable(tf.truncated_normal([1000], stddev=0.01), name='bd1')\n",
    "        dense_layer1 = tf.matmul(flattened, wd1) + bd1\n",
    "        dense_layer1 = tf.nn.relu(dense_layer1)\n",
    "\n",
    "        # another layer with softmax activations\n",
    "        wd2 = tf.Variable(tf.truncated_normal([1000, 10], stddev=0.03), name='wd2')\n",
    "        bd2 = tf.Variable(tf.truncated_normal([10], stddev=0.01), name='bd2')\n",
    "        dense_layer2 = tf.matmul(dense_layer1, wd2) + bd2\n",
    "        \n",
    "        y_ = tf.nn.softmax(dense_layer2)\n",
    "        #loss is cross_entropy loss\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dense_layer2, labels=y))\n",
    "        \n",
    "            \n",
    "            \n",
    "        #cross_entropy, y_ = self.CNN_Layer()\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        optimizer_gradient = AdamOptimizer_Bing(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "        optimizer = AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        grad = []\n",
    "\n",
    "        # setup the initialisation operator\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # initialise the variables\n",
    "            sess.run(init_op)\n",
    "            total_batch = int(len(dataset.train.labels) / batch_size)\n",
    "            count = 0\n",
    "            for epoch in range(self.torque):\n",
    "                avg_cost = 0\n",
    "                self.t += 1\n",
    "                count += 1\n",
    "\n",
    "                if count < self.torque:\n",
    "                    for i in range(total_batch):\n",
    "                        batch_x, batch_y = dataset.train.next_batch(batch_size=50)\n",
    "                        _,c = sess.run([optimizer,cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "                        avg_cost += c / total_batch\n",
    "\n",
    "                elif count == self.torque:\n",
    "                    '''\n",
    "                    #self.grad saved for belta computation. \n",
    "                    #It denotes in time t(update time), the gradient of local loss of local parameters\n",
    "\n",
    "                    '''\n",
    "\n",
    "                    for i in range(total_batch):\n",
    "                        batch_x, batch_y = dataset.train.next_batch(batch_size=batch_size)\n",
    "                        g,c = sess.run([optimizer_gradient,cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "                        gradient_temp = batch_gradient_collector(g[1])\n",
    "                        grad.append(gradient_temp)\n",
    "                        avg_cost += c / total_batch\n",
    "                    self.shape_list = parameter_shape(g[1])\n",
    "                    #Sum up gradients from each batch\n",
    "                    self.grad = np.array(grad).sum(axis = 0)\n",
    "                \n",
    "                self.w = batch_parameter_collector(g[1])\n",
    "                test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})   \n",
    "                self.history.append([avg_cost,test_acc, str(self.t)])\n",
    "\n",
    "\n",
    "            return self\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.torque = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'g' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-12c8db680ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-f308f4585926>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_parameter_collector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mavg_cost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'g' referenced before assignment"
     ]
    }
   ],
   "source": [
    "c.fit(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.7495106210898269, 0.9262, '1'], [0.1582654863587496, 0.9691, '2']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51200,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
