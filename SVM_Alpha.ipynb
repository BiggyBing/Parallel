{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sending date to engines\n",
    "c[0]['X'] = train_feature[:15000]\n",
    "c[1]['X'] = train_feature[15000:30000]\n",
    "c[2]['X'] = train_feature[30000:45000]\n",
    "c[3]['X'] = train_feature[45000:]\n",
    "\n",
    "c[0]['y'] = train_target[:15000]\n",
    "c[1]['y'] = train_target[15000:30000]\n",
    "c[2]['y'] = train_target[30000:45000]\n",
    "c[3]['y'] = train_target[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_1(X, y, w, eta = 1):\n",
    "    for i, x in enumerate(X):\n",
    "        if (y[i]*np.dot(X[i], w)) < 1:\n",
    "            grad = - y[i]*X[i] + eta * w\n",
    "        else:\n",
    "            grad = eta * w\n",
    "        \n",
    "        w = w - 0.001*grad\n",
    "    return w   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_2(X, Y):\n",
    "\n",
    "    w = np.zeros(len(X[0]))\n",
    "    eta = 1\n",
    "    epochs = 10000\n",
    "\n",
    "\n",
    "    for epoch in range(1,epochs):\n",
    "        for i, x in enumerate(X):\n",
    "            if (Y[i]*np.dot(X[i], w)) < 1:\n",
    "                w = w +  (X[i] * Y[i])\n",
    "            else:\n",
    "                w = w\n",
    "\n",
    "    return w\n",
    "\n",
    "w = SGD_2(train_feature[:1000],train_target[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(1000):\n",
    "    if train_target[i]*np.dot(w_hist[0],train_feature[i])>0:\n",
    "        result.append(0)\n",
    "    else:\n",
    "        result.append(1)\n",
    "score = np.array(result).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=50000.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(C=50000.,kernel='linear')\n",
    "clf.fit(train_feature[:1000],train_target[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502.4074320966727\n",
      "511.41409735519176\n",
      "516.8767925686861\n",
      "520.190006708386\n",
      "522.1995242066635\n",
      "523.4183283637104\n",
      "524.1575522122644\n",
      "524.605903019868\n",
      "524.8778347647902\n",
      "525.042765593844\n",
      "525.1427987017878\n",
      "525.2034703334556\n",
      "525.2402686188052\n",
      "525.262587349301\n",
      "525.2761240052704\n",
      "525.2843341969052\n",
      "525.2893138053641\n",
      "525.2923340150609\n",
      "525.2941658190433\n",
      "525.2952768365444\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros(784)\n",
    "w_hist = []\n",
    "for i in range(100):\n",
    "    w = SGD_1(train_feature[:1000]/255,train_target[:1000]/255,w,eta = 0.1)\n",
    "    loss = compute_loss(w,train_feature[:1000]/255,train_target[:1000]/255)\n",
    "    if i % 5 == 0:\n",
    "        w_hist.append(w)\n",
    "        print(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X, y, w, eta = 1):\n",
    "    for i, x in enumerate(X):\n",
    "        if (y[i]*np.dot(X[i], w)) < 1:\n",
    "            grad = eta * w + (np.dot(w,X[i]) - y[i])*X[i]\n",
    "        else:\n",
    "            grad = eta * w\n",
    "        \n",
    "        w = w - 0.01 * grad\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGD(X, y, w, eta = 1):\n",
    "    grad = 0\n",
    "    for i, x in enumerate(X):\n",
    "        if (y[i]*np.dot(X[i], w)) < 1:\n",
    "            grad += (np.dot(w,X[i]) - y[i])*X[i]\n",
    "        else:\n",
    "            grad += grad + eta * w\n",
    "    w = w - 0.001 * grad\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array([\n",
    "    [1,2],\n",
    "    [1,3],\n",
    "    [1,4],\n",
    "    [2,1],\n",
    "    [3,1],\n",
    "    [4,1]\n",
    "])\n",
    "target = np.array([1,1,1,-1,-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2 - line 7, retrieve w and c from aggregator \n",
    "def wc_from_eng(engine):\n",
    "    w = []\n",
    "    c = []\n",
    "    for i in range(len(engine.ids)):\n",
    "        w.append(engine[i]['svm.get_coef()'])\n",
    "        c.append(engine[i]['svm.Est_Resource()']) \n",
    "    return w,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2 - line 8, global parameter updata according to (5)\n",
    "data_size = np.array([150,150,150,150])\n",
    "def global_update(w_local,data_size):\n",
    "    temp = 0\n",
    "    for i in range(len(w_local)):\n",
    "        temp = temp + w_local[i] * data_size[i]\n",
    "    return temp/data_size.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve belta and grad to aggregator #Algorithm 2 - line 14\n",
    "def bg_from_eng(engine):\n",
    "    belta = []\n",
    "    grad = []\n",
    "    for i in range(len(engine.ids)):\n",
    "        belta.append(engine[i]['svm.belta'])\n",
    "        grad.append(engine[i]['svm.grad_t0']) \n",
    "    return belta,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2 - line 15\n",
    "def belta_update(belta_local,data_size):\n",
    "    temp = 0\n",
    "    for i in range(len(belta_local)):\n",
    "        temp = temp + np.array(belta_local[i]) * data_size[i]\n",
    "    return temp/data_size.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2 - line 16\n",
    "def grad_update(grad_local,data_size):\n",
    "    temp = 0\n",
    "    for i in range(len(grad_local)):\n",
    "        temp = temp + np.array(grad_local[i]) * data_size[i]\n",
    "    return temp/data_size.sum()\n",
    "\n",
    "def delta_update(local_grad, grad_aggregator, data_size):\n",
    "    delta_local = []\n",
    "    local_grad = np.array(local_grad) \n",
    "    grad_aggregator = np.array(grad_aggregator)\n",
    "    for item in local_grad:\n",
    "        temp_delta = np.linalg.norm(item - grad_aggregator)\n",
    "        delta_local.append(temp_delta)\n",
    "    return belta_update(delta_local,data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send torque and w to engines from aggregator\n",
    "def snd_to_eng(w,torque,engine):\n",
    "    for i in range(len(engine.ids)):\n",
    "        engine[i]['w_aggregator'] = w\n",
    "        engine[i]['torque_aggregator'] = torque[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm 2 - line 17, binary search for new torque\n",
    "def G(torque, delta, belta, eta, phi):\n",
    "    h = delta* (pow((eta*belta + 1),torque)-1)/belta - eta*delta*torque\n",
    "    G = torque*(eta*(1-belta*eta/2)-phi*h/torque)\n",
    "    return G\n",
    "\n",
    "def binary_search(torque,delta, belta, gamma,phi,eta = 0.0001):\n",
    "    upper_bound = int(gamma * torque)\n",
    "    G_list = []\n",
    "    for i in range(upper_bound):\n",
    "        torque_try = i + 1\n",
    "        G_list.append(G(torque_try, delta, belta, eta, phi))\n",
    "    torque_star = np.argmax(np.array(G_list)) + 1\n",
    "    return torque_star, G_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(w, feature, target, eta = 1):\n",
    "    loss = 0\n",
    "    for i, x in enumerate(feature):\n",
    "        if (target[i]*np.dot(feature[i], w)) < 1:\n",
    "            loss += 0.5*np.linalg.norm(w) + 0.5*(1-target[i]*np.dot(w,feature[i]))**2\n",
    "        else:\n",
    "            loss += 0.5*np.linalg.norm(w)\n",
    "    return loss   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t = 0\n",
    "        self.t_0 = 0\n",
    "        self.grad = 0\n",
    "        self.learning_rate = 0.0001\n",
    "        self.torque = 0\n",
    "        self.belta = 0\n",
    "        self.resource = 0\n",
    "        self.grad_t0 = 0\n",
    "        self.history = []\n",
    "        self.w = 0\n",
    "        self.w_hat = 0\n",
    "        self.w_t0 = 0\n",
    "    \n",
    "    def Rec_from_Agg(self, w_global, torque_global):\n",
    "        self.w_t0 = w_global\n",
    "        self.w_hat = w_global\n",
    "        self.torque = torque_global\n",
    "    def Snd_to_Agg(self):\n",
    "        if self.t_0 > 0:\n",
    "            return w,self.resource, self.belta, self.grad_t0\n",
    "        else:\n",
    "            return w,self.resource\n",
    "    def aa(self):\n",
    "        self.w = self.w_hat\n",
    "    \n",
    "    def Est_Resource(self):\n",
    "        return self.resource\n",
    "\n",
    "    def get_coef(self):\n",
    "        return self.w\n",
    "    \n",
    "    def set_coef(self, w_global):\n",
    "        self.w = w_global\n",
    "        \n",
    "    def Est_Belta(self ,X, y):\n",
    "        grad_global_parameter = np.dot((np.dot(X ,self.w_hat)-y), X)\n",
    "        # In time t, the gradient of local loss of global parameters\n",
    "        self.grad_t0 = grad_global_parameter\n",
    "        self.belta = np.linalg.norm(self.grad - grad_global_parameter)/np.linalg.norm(self.w - self.w_hat)\n",
    "        \n",
    "    def time_record(self):\n",
    "        self.t_0 = self.t\n",
    "        \n",
    "    def fit(self, X, y, optimizer = 'SGD'):\n",
    "        self.w = self.w_hat #updata local w by aggregator \n",
    "        \n",
    "        count = 0\n",
    "        tic = time.time()\n",
    "        for i in range(self.torque): \n",
    "            \n",
    "            loss = compute_loss(self.w, X, y)\n",
    "            self.history.append([loss,str(self.t)])#record loss history from t=1, t=0(w intialization) not included\n",
    "            \n",
    "            self.t += 1\n",
    "            if optimizer == 'SGD':\n",
    "                self.w = SGD(X, y, self.w, eta = 1)\n",
    "            elif optimizer == 'DGD':\n",
    "                self.w = BGD(X, y, self.w, eta = 1)\n",
    "            else:\n",
    "                raise ValueError('optimizer must be \\'SGD\\' or \\'DGD\\'!')\n",
    "            \n",
    "            count += 1\n",
    "            if count < self.torque:\n",
    "                self.w_hat = self.w\n",
    "            elif count == self.torque:\n",
    "                '''\n",
    "                #self.grad saved for belta computation. \n",
    "                #It denotes in time t(update time), the gradient of local loss of local parameters\n",
    "                \n",
    "                '''\n",
    "                self.grad = grad  \n",
    "            \n",
    "            \n",
    "        toc = time.time()\n",
    "        self.resource = toc - tic\n",
    "        \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM()\n",
    "dview.push(dict(SVM=SVM,compute_loss = compute_loss,SGD=SGD,BGD=BGD))\n",
    "#sending LinearRegression object to engines\n",
    "dview['svm'] = svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iteration and loss is: 29999.500000\n",
      "1 iteration and loss is: 67967311233.082985\n",
      "New torque is: 1\n",
      "2 iteration and loss is: 61170580109.818970\n",
      "New torque is: 1\n",
      "3 iteration and loss is: 55053522098.818710\n",
      "New torque is: 1\n",
      "4 iteration and loss is: 49548169888.971321\n",
      "New torque is: 1\n",
      "5 iteration and loss is: 44593352900.124237\n",
      "New torque is: 1\n",
      "6 iteration and loss is: 40134017610.024048\n",
      "New torque is: 1\n",
      "7 iteration and loss is: 36120615849.016586\n",
      "New torque is: 1\n",
      "8 iteration and loss is: 32508554264.126392\n",
      "New torque is: 1\n",
      "9 iteration and loss is: 29257698837.700439\n",
      "New torque is: 1\n",
      "10 iteration and loss is: 26331928953.950726\n",
      "New torque is: 1\n",
      "11 iteration and loss is: 23698736058.575405\n",
      "New torque is: 1\n",
      "12 iteration and loss is: 21328862452.713509\n",
      "New torque is: 1\n",
      "13 iteration and loss is: 19195976207.431736\n",
      "New torque is: 1\n",
      "14 iteration and loss is: 17276378586.715393\n",
      "New torque is: 1\n",
      "15 iteration and loss is: 15548740728.014320\n",
      "New torque is: 1\n",
      "16 iteration and loss is: 13993866655.226645\n",
      "New torque is: 1\n",
      "17 iteration and loss is: 12594479989.707998\n",
      "New torque is: 1\n",
      "18 iteration and loss is: 11335031990.743603\n",
      "New torque is: 1\n",
      "19 iteration and loss is: 10201528791.655613\n",
      "New torque is: 1\n",
      "20 iteration and loss is: 9181375912.502647\n",
      "New torque is: 1\n",
      "21 iteration and loss is: 8263238321.243927\n",
      "New torque is: 1\n",
      "22 iteration and loss is: 7436914489.107445\n",
      "New torque is: 1\n",
      "23 iteration and loss is: 6693223040.202504\n",
      "New torque is: 1\n",
      "24 iteration and loss is: 6023900736.191144\n",
      "New torque is: 1\n",
      "25 iteration and loss is: 5421510662.569510\n",
      "New torque is: 1\n",
      "26 iteration and loss is: 4879359596.313795\n",
      "New torque is: 1\n",
      "27 iteration and loss is: 4391423636.676622\n",
      "New torque is: 1\n",
      "28 iteration and loss is: 3952281273.013318\n",
      "New torque is: 1\n",
      "29 iteration and loss is: 3557053145.709062\n",
      "New torque is: 1\n",
      "30 iteration and loss is: 3201347831.141701\n",
      "New torque is: 1\n",
      "31 iteration and loss is: 2881213048.029393\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "t = 0\n",
    "s = 0\n",
    "b = 0 # aggregator consumption\n",
    "R = 50\n",
    "gamma = 10\n",
    "phi = 0.2\n",
    "w_aggregator = np.zeros(784)\n",
    "torque_aggregator = [1,1,1,1]\n",
    "stop = False\n",
    "loss_hist = []\n",
    "while True:\n",
    "    tic = time.time()\n",
    "    snd_to_eng(w_aggregator,torque_aggregator,c)\n",
    "    loss = compute_loss(w_aggregator,train_feature,train_target)\n",
    "    loss_hist.append(loss)\n",
    "    print ('%d iteration and loss is: %f'%(t,loss))\n",
    "    t_0 = t\n",
    "    t = t + torque_aggregator[0]\n",
    "    dview.execute(\"\"\"\n",
    "import numpy as np\n",
    "import time    \n",
    "svm.Rec_from_Agg(w_aggregator, torque_aggregator)\n",
    "svm.time_record()\n",
    "if svm.t > 0:\n",
    "    svm.Est_Belta(X,y)\n",
    "svm.fit(X,y,optimizer = 'DGD')\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    w_local, c_local = wc_from_eng(c)\n",
    "    #calculate local consumption per iteration\n",
    "    c_per = np.array(c_local)/torque_aggregator[0]\n",
    "    w_aggregator = global_update(w_local, data_size)\n",
    "    if stop:\n",
    "        w_final = w_aggregator\n",
    "        break\n",
    "    #c_local.sum() equal to c*t\n",
    "    s = s + np.array(c_local).sum()+ b\n",
    "    \n",
    "    if t_0 > 0:\n",
    "        belta_local, grad_local = bg_from_eng(c)\n",
    "        belta_aggregator = belta_update(belta_local,data_size)\n",
    "        grad_aggregator = grad_update(grad_local,data_size)\n",
    "        delta_aggregator = delta_update(grad_local, grad_aggregator, data_size)\n",
    "        torque_update, G_list = binary_search(torque_aggregator[0], delta_aggregator, belta_aggregator, gamma, phi)\n",
    "        torque_aggregator = [torque_update for i in range(len(c.ids))]\n",
    "        print('New torque is:',torque_aggregator[0])\n",
    "    \n",
    "    toc = time.time()\n",
    "    b = toc - tic\n",
    "    temp = s + torque_aggregator[0]*c_per.sum() + b\n",
    "    if temp >= R:\n",
    "        torque_max = (R-b-s)/c_per.sum()\n",
    "        G_list = np.array(G_list)\n",
    "        G_min = G_list.min()\n",
    "        for i, item in enumerate(G_list):\n",
    "            if item >= torque_max:\n",
    "                itme = G_min\n",
    "        torque_updata = np.argmax(G_list) + 1\n",
    "        torque_aggregator = [torque_update for i in range(len(c.ids))]\n",
    "        stop = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.30389881,  0.28289024]),\n",
       " array([-0.30389881,  0.28289024]),\n",
       " array([-0.30389881,  0.28289024]),\n",
       " array([-0.30389821,  0.28288954])]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.58876117  3.17458055 11.11863105]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [-2,4,-1],\n",
    "    [4,1,-1],\n",
    "    [1, 6, -1],\n",
    "    [2, 4, -1],\n",
    "    [6, 2, -1],\n",
    "\n",
    "])\n",
    "\n",
    "y = np.array([-1,-1,1,1,1])\n",
    "\n",
    "def svm_sgd(X, Y):\n",
    "\n",
    "    w = np.zeros(len(X[0]))\n",
    "    eta = 1\n",
    "    epochs = 100000\n",
    "\n",
    "\n",
    "    for epoch in range(1,epochs):\n",
    "        for i, x in enumerate(X):\n",
    "            if (Y[i]*np.dot(X[i], w)) < 1:\n",
    "                w = w + eta * ( (X[i] * Y[i]) + (-2  *(1/epoch)* w) )\n",
    "            else:\n",
    "                w = w + eta * (-2  *(1/epoch)* w)\n",
    "\n",
    "    return w\n",
    "\n",
    "a = svm_sgd(X,y)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('mnist_train.csv')\n",
    "test = pd.read_csv('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = train[:,1:785]\n",
    "train_label = train[:,0]\n",
    "train_target = np.zeros(len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_label)):\n",
    "    if train_label[i]%2 == 0:\n",
    "        train_target[i] = 1\n",
    "    else:\n",
    "        train_target[i] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1., -1., ..., -1.,  1.,  1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
