{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import numpy as np\n",
    "import ipyparallel as ipp\n",
    "from cnn import CNN\n",
    "import tensorflow as tf\n",
    "from AggregatorFunc import snd_to_eng, wc_from_eng, global_update, bg_from_eng, belta_update, grad_update, delta_update, binary_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dview = c[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_shape(gradient):\n",
    "    '''\n",
    "    Record the parameters original shape\n",
    "    Input: the return value of optimizer.compute_gradients (many matrices)\n",
    "    Output: A list of shape\n",
    "    \n",
    "    '''\n",
    "    gradient = np.array(gradient)\n",
    "    #shape_list record parameter shapes of each layer\n",
    "    shape_list = []\n",
    "    for i in range(gradient.shape[0]):\n",
    "        grad_temp = gradient[i,0].flatten()\n",
    "        shape_list.append(['Layer_'+str(i+1), gradient[i,0].shape, grad_temp.shape[0]])\n",
    "        \n",
    "    return shape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_collector(gradient):\n",
    "    '''\n",
    "    Collect the gradient of each batch\n",
    "    Input: the return value of optimizer.compute_gradients (many matrices)\n",
    "    Output: the sum of gradients within one epoch as a vector\n",
    "    \n",
    "    '''\n",
    "    gradient = np.array(gradient)\n",
    "    #shape_list record parameter shapes of each layer\n",
    "    gradient_vector = []\n",
    "    for i in range(gradient.shape[0]):\n",
    "        grad_temp = gradient[i,0].flatten()\n",
    "        gradient_vector.append(grad_temp)\n",
    "        \n",
    "    return np.array(gradient_vector)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_parameter_collector(gradient):\n",
    "    '''\n",
    "    Collect the gradient of each batch\n",
    "    Input: the return value of optimizer.compute_gradients (many matrices)\n",
    "    Output: the sum of gradients within one epoch as a vector\n",
    "    \n",
    "    '''\n",
    "    gradient = np.array(gradient)\n",
    "    #shape_list record parameter shapes of each layer\n",
    "    parameter_vector = []\n",
    "    for i in range(gradient.shape[0]):\n",
    "        grad_temp = gradient[i,1].flatten()\n",
    "        parameter_vector.append(grad_temp)\n",
    "        \n",
    "    return np.array(parameter_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_conv_layer(input_data, num_input_channels, num_filters, filter_shape, pool_shape, name):\n",
    "    # setup the filter input shape for tf.nn.conv_2d\n",
    "    conv_filt_shape = [filter_shape[0], filter_shape[1], num_input_channels,\n",
    "                      num_filters]\n",
    "\n",
    "    # initialise weights and bias for the filter\n",
    "    weights = tf.Variable(tf.truncated_normal(conv_filt_shape, stddev=0.03),\n",
    "                                      name=name+'_W')\n",
    "    bias = tf.Variable(tf.truncated_normal([num_filters]), name=name+'_b')\n",
    "\n",
    "    # setup the convolutional layer operation\n",
    "    out_layer = tf.nn.conv2d(input_data, weights, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # add the bias\n",
    "    out_layer += bias\n",
    "\n",
    "    # apply a ReLU non-linear activation\n",
    "    out_layer = tf.nn.relu(out_layer)\n",
    "\n",
    "    # now perform max pooling\n",
    "    ksize = [1, pool_shape[0], pool_shape[1], 1]\n",
    "    strides = [1, 2, 2, 1]\n",
    "    out_layer = tf.nn.max_pool(out_layer, ksize=ksize, strides=strides, \n",
    "                               padding='SAME')\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer_Bing(AdamOptimizer):\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
    "               use_locking=False, name=\"Adam\"):\n",
    "\n",
    "        super(AdamOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        # Tensor versions of the constructor arguments, created in _prepare().\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = None\n",
    "        self._beta2_t = None\n",
    "        self._epsilon_t = None\n",
    "\n",
    "        # Created in SparseApply if needed.\n",
    "        self._updated_lr = None\n",
    "    \n",
    "    def minimize(self, loss, global_step=None, var_list=None,\n",
    "               gate_gradients=1, aggregation_method=None,\n",
    "               colocate_gradients_with_ops=False, name=None,\n",
    "               grad_loss=None):\n",
    "        \"\"\"\n",
    "        The same as function minimize, but return the result of compute_gradients\n",
    "        Created by: Big Bing in 7/28 \n",
    "        Purpose: To realize parallel computing(communicate gradient)\n",
    "        \"\"\"\n",
    "        grads_and_vars = self.compute_gradients(\n",
    "            loss, var_list=var_list, gate_gradients=gate_gradients,\n",
    "            aggregation_method=aggregation_method,\n",
    "            colocate_gradients_with_ops=colocate_gradients_with_ops,\n",
    "            grad_loss=grad_loss)[-8:]\n",
    "\n",
    "        vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n",
    "        if not vars_with_grad:\n",
    "          raise ValueError(\n",
    "              \"No gradients provided for any variable, check your graph for ops\"\n",
    "              \" that do not support gradients, between variables %s and loss %s.\" %\n",
    "              ([str(v) for _, v in grads_and_vars], loss))\n",
    "        #self.apply_gradients(grads_and_vars, global_step=global_step, name=name)\n",
    "\n",
    "        return self.apply_gradients(grads_and_vars, global_step=global_step, name=name),grads_and_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier', 'D:\\\\GitHub\\\\Parallel\\\\classifier']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dview.map(os.chdir, ['D:/GitHub/Parallel/classifier']*len(c.ids))\n",
    "print(dview.apply_sync(os.getcwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sending date to engines\n",
    "for i in range(len(c.ids)):\n",
    "    c[i]['mnist'] = mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "s = 0\n",
    "b = 0 # aggregator consumption\n",
    "R = 15\n",
    "\n",
    "# Python optimisation variables\n",
    "batch_size = 50\n",
    "layer_size = [[1, 32, [5, 5], [2, 2]],\n",
    "                [32, 64, [5, 5], [2, 2]],\n",
    "                1000,\n",
    "                10]\n",
    "\n",
    "data_size = np.array([15000,15000])\n",
    "gamma = 10\n",
    "phi = 0.2\n",
    "torque_aggregator = 1\n",
    "stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustment needed when change data\n",
    "parameter_length_list = [32*5*5*1,32,32*64*5*5,64,1000*7*7*64,1000,1000*10,10]\n",
    "w_aggregator = cnn_parameter_initial(parameter_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(layer_size)\n",
    "dview.push(dict(CNN = CNN))\n",
    "#sending LinearRegression object to engines\n",
    "dview['cnn'] = cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        tic = time.time()\n",
    "        snd_to_eng(w_aggregator,torque_aggregator,c)\n",
    "        t_0 = t\n",
    "        t = t + torque_aggregator\n",
    "        dview.execute(\"\"\"\n",
    "import numpy as np\n",
    "import time    \n",
    "cnn.Rec_from_Agg(w_aggregator, torque_aggregator)\n",
    "cnn.time_record()\n",
    "if cnn.t > 0:\n",
    "    cnn.Est_Belta(mnist)\n",
    "svm.fit(mnist)\n",
    "        \"\"\")\n",
    "        \n",
    "        \n",
    "        w_local, c_local = wc_from_eng(c,'cnn')\n",
    "        w_local = flatten_matrix(w_local)\n",
    "        #calculate local consumption per iteration\n",
    "        c_per = np.array(c_local)/torque_aggregator\n",
    "        w_aggregator = global_update(w_local, data_size)\n",
    "        w_aggregator = wrangle_matrix(w_aggregator, parameter_length_list)\n",
    "        if stop:\n",
    "            w_final = w_aggregator\n",
    "            break\n",
    "        #c_local.sum() equal to c*t\n",
    "        s = s + np.array(c_local).sum()+ b\n",
    "        \n",
    "        if t_0 > 0:\n",
    "            belta_local, grad_local = bg_from_eng(c,'cnn')\n",
    "            grad_local = flatten_matrix(grad_local)\n",
    "            belta_aggregator = belta_update(belta_local,data_size)\n",
    "            grad_aggregator = grad_update(grad_local,data_size)\n",
    "            delta_aggregator = delta_update(grad_local, grad_aggregator, data_size)\n",
    "            torque_aggregator, G_list = binary_search(torque_aggregator, delta_aggregator, belta_aggregator, gamma, phi)\n",
    "            print('New torque is:',torque_aggregator)\n",
    "        \n",
    "        toc = time.time()\n",
    "        b = toc - tic\n",
    "        temp = s + torque_aggregator*c_per.sum() + b\n",
    "        if temp >= R:\n",
    "            torque_max = (R-b-s)/c_per.sum()\n",
    "            G_list = np.array(G_list)\n",
    "            G_min = G_list.min()\n",
    "            for i, item in enumerate(G_list):\n",
    "                if item >= torque_max:\n",
    "                    itme = G_min\n",
    "            torque_aggregator = np.argmax(G_list) + 1\n",
    "            stop = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The padding type is in consistent with pooling strided defined in create_new_conv_layer()\n",
    "def conv_output_size(input_size, filter_size, stride, padding = 'Same'):\n",
    "    if padding == 'Same':\n",
    "        output_size = input_size\n",
    "    else:\n",
    "        output_size = int((input_size - filter_size)/stride) + 1\n",
    "    return output_size\n",
    "#The strides of pooling is defaulted to be 2 in consistent with pooling strided defined in create_new_conv_layer()\n",
    "def pooling_output_size(input_size, filter_size, stride=2):\n",
    "    return int((input_size - filter_size)/stride) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_matrix(matrix):\n",
    "    '''\n",
    "    Flatten parameter matrix recieve from classifier objects.\n",
    "    Parameter recieved from cnn object have a shape (#number of parameter matrix, # of paramter each matrix)\n",
    "    \n",
    "    matrix: paramter and gradient matrix of cnn\n",
    "    output: a vector \n",
    "    '''\n",
    "    temp = []\n",
    "    for i in range(len(matrix)):\n",
    "        temp.extend(matrix[i])\n",
    "    return temp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_matrix(vector, parameter_length_list):\n",
    "    '''\n",
    "    Reverse function of flatten_matrix.\n",
    "    vector to matrix\n",
    "    Var:\n",
    "        vector: A vector like parameter list\n",
    "        parameter_length_list: A list that saves parameter length of each layer\n",
    "    '''\n",
    "    matrix = []\n",
    "    flag = 0\n",
    "    for i in range(len(parameter_length_list)):\n",
    "            temp = vector[flag:parameter_length_list[i]]\n",
    "            matrix.append(temp)\n",
    "            flag += parameter_length_list[i]\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
